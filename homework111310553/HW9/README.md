# 線性代數觀念筆記 (Linear Algebra Concepts)

## 1. 核心定義：線性與代數
* **「線性」(Linear) 的意義**：
    指的是變換過程滿足以下兩個性質：
    1.  **加法性**：$f(u + v) = f(u) + f(v)$
    2.  **齊次性**：$f(cu) = cf(u)$
    在幾何上，這意味著原點固定，且所有直線在變換後依然是直線，空間網格保持平行且等距。

* **為何稱為「代數」(Algebra)**：
    「代數」源於阿拉伯語 *al-jabr*，意為「破碎部分的重組」。在數學中，它指的是研究**運算規則**的學問。線性代數即是研究向量與矩陣在加法與乘法運算下的結構。

---

## 2. 空間與向量空間
* **數學中的「空間」(Space)**：
    空間是指一個**集合 (Set)** 加上定義在其上的**結構 (Structure)**（例如運算規則或距離定義）。
* **「向量空間」為何稱為空間**：
    因為它提供了一個「場域」，讓向量可以在其中進行線性組合（加法與純量乘法）。只要滿足 8 條公理（如封閉性、結合律、存在單位元等），這個集合就能被稱為空間。

---

## 3. 矩陣與幾何變換
* **矩陣與向量的關係**：
    * **向量**：空間中的一個點或方向。
    * **矩陣**：一個**線性算子 (Operator)** 或函數。
    * **$Ax = b$**：代表將向量 $x$ 透過矩陣 $A$ 的規則，映射（移動）到新位置 $b$。
* **矩陣的意義**：
    矩陣的每一欄（Column）代表變換後，原始基底向量（如 $\hat{i}, \hat{j}$）落下的新座標。

### 2D/3D 幾何操作
| 操作 | 矩陣實現方式 |
| :--- | :--- |
| **縮放 (Scaling)** | 使用對角矩陣，對角線值為倍率。 |
| **旋轉 (Rotation)** | 使用三角函數，如 $\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$。 |
| **平移 (Translation)** | 無法用低維線性矩陣表達。需使用**齊次坐標 (Homogeneous Coordinates)**，在 $n+1$ 維空間中用矩陣表達。 |



---

## 4. 行列式 (Determinant)
* **意義**：
    行列式代表變換後的**「體積縮放比例」**。若 $\det(A) = 0$，表示空間被壓縮到更低維度（體積消失），矩陣不可逆。
* **遞迴計算 (Laplace Expansion)**：
    透過降階公式：$\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(M_{ij})$，其中 $M_{ij}$ 是餘因子矩陣。
* **對角化快速計算**：
    若 $A = PDP^{-1}$，則 $\det(A) = \det(D)$，即所有特徵值的乘積 ($\prod \lambda_i$)。
* **LU 分解計算**：
    因為 $\det(A) = \det(L) \times \det(U)$，而 $L$ 的對角線通常為 $1$，故 $\det(A) = \prod u_{ii}$（$U$ 的對角線乘積）。

---

## 5. 特徵系統與矩陣分解

### 特徵值與特徵向量
* **意義**：當矩陣 $A$ 作用於向量 $v$ 時，若 $v$ 僅發生伸縮而沒有轉向，則 $v$ 為特徵向量，縮放倍率 $\lambda$ 為特徵值 ($Ax = \lambda x$)。
* **用途**：對角化矩陣、簡化運算、動態系統分析（如 Google PageRank 演算法）。

### QR 分解與特徵值疊代
* **QR 分解**：將矩陣分解為 $Q$（正交矩陣，保持長度與角度）與 $R$（上三角矩陣）。
* **求特徵值**：反覆進行 $A_k = Q_k R_k \rightarrow A_{k+1} = R_k Q_k$，矩陣會趨向對角化，對角線即為特徵值。

### SVD (奇異值分解)
* **定義**：$A = U\Sigma V^T$。適用於任何矩陣。
* **與特徵值關係**：$\Sigma$ 的值（奇異值）是 $A^T A$ 特徵值的平方根。
* **PCA (主成分分析)**：PCA 的本質是對數據的協方差矩陣做特徵值分解，或直接對中心化數據做 SVD。SVD 能夠找出數據變異量最大的方向，達成降維。
